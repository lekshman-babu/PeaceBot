{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName1='DataSet1'\n",
    "fileName2='DataSet2'\n",
    "fileName3='DataSet3'\n",
    "dataSetRoot='DataSets/'\n",
    "intent='intents.json'\n",
    "header=['question','reply']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataPreprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataSet1=pd.read_csv(dataSetRoot+'RawDataSet1',encoding='utf-8')\n",
    "# preprocessedData=[]\n",
    "# for j in range(len(dataSet1['conversations'])):\n",
    "#     convo={}\n",
    "#     flag=0\n",
    "#     for i in dataSet1['conversations'][j].split('\\n'):\n",
    "#         if i[-1]==']':\n",
    "#             content=eval(i[:-1])['value']\n",
    "#             key=eval(i[:-1])['from']\n",
    "#         else:\n",
    "#             content=eval(i[1:])['value']\n",
    "#             key=eval(i[1:])['from']\n",
    "        \n",
    "#         content=re.sub(r'\\!|\\,|\\$|\\%|\\&|\\(|\\)|\\*|\\+|\\-|\\/|\\:|\\;|\\[|\\]|\\_|\\‚Äú|\\‚Äù|\\‚Ä¶|üåà|üåü|üå†|üå∫|üåª|üéâ|üí´|üìù|üôÅ',' ', content)\n",
    "#         content=re.sub(r'\\.',\" . \",content)\n",
    "#         content=re.sub(r\"\\?\",\" ? \",content)\n",
    "#         content=re.sub(r\"\\'\",\" ' \",content)\n",
    "#         content=re.sub(r\"\\\"\",\" \\\" \",content)\n",
    "#         content=re.sub(r'\\s+',\" \",content)\n",
    "#         if key=='human':\n",
    "#             convo['question']=content.lower()\n",
    "#         else:\n",
    "#             convo['reply']=content.lower()\n",
    "#             preprocessedData.append(dict(convo))\n",
    "# convo.clear()\n",
    "# with open(dataSetRoot+fileName1,'w',encoding='utf=8',newline=\"\") as csvFile:\n",
    "#     writer=csv.DictWriter(csvFile,fieldnames=header)\n",
    "#     writer.writeheader()\n",
    "#     writer.writerows(preprocessedData)\n",
    "# preprocessedData=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(dataSetRoot+'RawDataSet2','r') as file:\n",
    "#     a=file.read()\n",
    "# preProcessedData=[]\n",
    "# for i in a.split('\\n'):\n",
    "#     question=eval(i)['Context']\n",
    "#     question=re.sub(r',',' ',question)\n",
    "#     question=re.sub(r\"\\?\",\" ? \",question)\n",
    "#     question=re.sub(r\"\\.\",\" . \",question)\n",
    "#     question=re.sub(r\"\\!\",\" ! \",question)\n",
    "#     question=re.sub(r\"\\'\",\" ' \",question)\n",
    "#     question=re.sub(r'\\s+',' ',question)\n",
    "\n",
    "#     reply=eval(i)['Response']\n",
    "#     reply=re.sub(r',',' ',reply)\n",
    "#     reply=re.sub(r\"\\?\",\" ? \",reply)\n",
    "#     reply=re.sub(r\"\\.\",\" . \",reply)\n",
    "#     reply=re.sub(r\"\\!\",\" ! \",reply)\n",
    "#     reply=re.sub(r\"\\'\",\" ' \",reply)\n",
    "#     reply=re.sub(r'\\s+',' ',reply)\n",
    "#     preProcessedData.append([question.lower(),reply.lower()])\n",
    "# with open(dataSetRoot+fileName2,'w',encoding='utf=8',newline=\"\") as csvFile:\n",
    "#     writer=csv.writer(csvFile)\n",
    "#     writer.writerow(header)\n",
    "#     writer.writerows(preProcessedData)\n",
    "# preProcessedData=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(dataSetRoot+intent,'r') as file:\n",
    "#     data=json.load(file)\n",
    "# preProcessesData=[]\n",
    "# for i in data['intents']:\n",
    "#     for question,reply in zip(i['patterns'],i['responses']):\n",
    "#         question=re.sub(r',',' ',question)\n",
    "#         question=re.sub(r\"\\?\",\" ? \",question)\n",
    "#         question=re.sub(r\"\\.\",\" . \",question)\n",
    "#         question=re.sub(r\"\\!\",\" ! \",question)\n",
    "#         question=re.sub(r\"\\'\",\" ' \",question)\n",
    "#         question=re.sub(r'\\s+',' ',question)\n",
    "\n",
    "#         reply=re.sub(r',',' ',reply)\n",
    "#         reply=re.sub(r\"\\?\",\" ? \",reply)\n",
    "#         reply=re.sub(r\"\\.\",\" . \",reply)\n",
    "#         reply=re.sub(r\"\\!\",\" ! \",reply)\n",
    "#         reply=re.sub(r\"\\'\",\" ' \",reply)\n",
    "#         reply=re.sub(r'\\s+',' ',reply)\n",
    "\n",
    "#         preProcessesData.append([question.lower(),reply.lower()])\n",
    "# with open(dataSetRoot+fileName3,'w',encoding='utf=8',newline=\"\") as csvFile:\n",
    "#     writer=csv.writer(csvFile)\n",
    "#     writer.writerow(header)\n",
    "#     writer.writerows(preProcessesData)\n",
    "# preProcessesData=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=pd.read_csv(dataSetRoot+'dialogs.csv')\n",
    "# preProccessedData=[]\n",
    "# for i in range(len(a)):\n",
    "#     question=a['Input'][i]\n",
    "#     question=re.sub(r',',' ',question)\n",
    "#     question=re.sub(r\"\\?\",\" ? \",question)\n",
    "#     question=re.sub(r\"\\.\",\" . \",question)\n",
    "#     question=re.sub(r\"\\!\",\" ! \",question)\n",
    "#     question=re.sub(r\"\\'\",\" ' \",question)\n",
    "#     question=re.sub(r'\\s+',' ',question)\n",
    "#     reply=a['Output'][i]\n",
    "#     reply=re.sub(r',',' ',reply)\n",
    "#     reply=re.sub(r\"\\?\",\" ? \",reply)\n",
    "#     reply=re.sub(r\"\\.\",\" . \",reply)\n",
    "#     reply=re.sub(r\"\\!\",\" ! \",reply)\n",
    "#     reply=re.sub(r\"\\'\",\" ' \",reply)\n",
    "#     reply=re.sub(r'\\s+',' ',reply)\n",
    "\n",
    "#     preProccessedData.append([question.lower(),reply.lower()])\n",
    "\n",
    "# with open(dataSetRoot+'DataSet4','w',encoding='utf=8',newline=\"\") as csvFile:\n",
    "#     writer=csv.writer(csvFile)\n",
    "#     writer.writerow(header)\n",
    "#     writer.writerows(preProccessedData)\n",
    "# preProccessedData=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AFTER PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import Model,Sequential\n",
    "from keras.layers import Embedding,Dense,Softmax,Dropout,LayerNormalization,Layer\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.optimizers.schedules.learning_rate_schedule import ExponentialDecay\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import os\n",
    "from keras.preprocessing.text import tokenizer_from_json\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet1=pd.read_csv(dataSetRoot+fileName1)\n",
    "dataSet=dataSet1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataSet2=pd.read_csv(dataSetRoot+fileName2)\n",
    "# # dataSet=dataSet2\n",
    "# dataSet3=pd.read_csv(dataSetRoot+fileName3)\n",
    "# # dataSet=dataSet3\n",
    "# dataSet4=pd.read_csv(dataSetRoot+'DataSet4')\n",
    "# # dataSet=dataSet4\n",
    "# dataSet=pd.concat([dataSet2,dataSet3,dataSet4],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=dataSet['question'].tolist()\n",
    "reply=dataSet['reply'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionTrain,questionTest,replyTrain,replyTest=train_test_split(question,reply,test_size=0.0001,shuffle=False)\n",
    "# questionTrain,questionValidate,replyTrain,replyValidate=train_test_split(questionTemp,replyTemp,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(807004, 81)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questionTrain),len(questionTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization and Sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionToken=Tokenizer(num_words=10000,oov_token=\"<unk>\",filters='\"#$%&()*+-/:;<=>@![\\\\]^_`{|}~\\t\\n.,')\n",
    "\n",
    "questionToken.fit_on_texts(questionTrain)\n",
    "questionTrainSequences=questionToken.texts_to_sequences(questionTrain)\n",
    "# questionValidateSequences=questionToken.texts_to_sequences(questionValidate)\n",
    "questionTestSequences=questionToken.texts_to_sequences(questionTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagging(reply):\n",
    "    for i in range(len(reply)):\n",
    "        reply[i]='<start> '+reply[i]+' <end>'\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "taggedReplyTrain=tagging(replyTrain)\n",
    "# taggedReplyValidate=tagging(replyValidate)\n",
    "taggedReplyTest=tagging(replyTest)\n",
    "\n",
    "replyToken=Tokenizer(num_words=10000,oov_token=\"<unk>\",filters='\"#$%&()*+-/:;=@[\\\\]!^_`{|}~\\t\\n.,')\n",
    "\n",
    "replyToken.fit_on_texts(replyTrain)\n",
    "replyTrainSequences=replyToken.texts_to_sequences(taggedReplyTrain)\n",
    "# replyValidateSequences=replyToken.texts_to_sequences(taggedReplyValidate)\n",
    "replyTestSequences=replyToken.texts_to_sequences(taggedReplyTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxEncodingLen=len(max(questionTrainSequences,key=len))\n",
    "# maxDecodingLen=len(max(replyTrainSequences,key=len))\n",
    "maxEncodingLen=200\n",
    "maxDecodingLen=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxEncodingLen,maxDecodingLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoderTrainPaddedSequence=pad_sequences(sequences=questionTrainSequences,maxlen=maxEncodingLen,padding='post',truncating='post')\n",
    "decoderTrainPaddedSequence=pad_sequences(sequences=replyTrainSequences,maxlen=maxDecodingLen,padding='post',truncating='post')\n",
    "# decoderTrainOutputPaddedSequence=pad_sequences(sequences=decoderTrainOutput,maxlen=maxDecodingOutputLen,padding='post',truncating='post')\n",
    "\n",
    "# encoderValidatePaddedSequence=pad_sequences(sequences=questionValidateSequences,maxlen=maxEncodingLen,padding='post',truncating='post')\n",
    "# decoderValidatePaddedSequence=pad_sequences(sequences=replyValidateSequences,maxlen=maxDecodingLen,padding='post',truncating='post')\n",
    "# # decoderValidateOutputPaddedSequence=pad_sequences(sequences=decoderValidateOutput,maxlen=maxDecodingOutputLen,padding='post',truncating='post')\n",
    "\n",
    "encoderTestPaddedSequence=pad_sequences(sequences=questionTestSequences,maxlen=maxEncodingLen,padding='post',truncating='post')\n",
    "decoderTestPaddedSequence=pad_sequences(sequences=replyTestSequences,maxlen=maxDecodingLen,padding='post',truncating='post')\n",
    "# decoderTestOutputPaddedSequence=pad_sequences(sequences=decoderTestOutput,maxlen=maxDecodingOutputLen,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maskEncoder=tf.cast(tf.math.not_equal(encoderTrainPaddedSequence[:20],0),tf.float32)\n",
    "# maskEncoder=maskEncoder[:,tf.newaxis,tf.newaxis,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maskDecoder=tf.cast(tf.math.not_equal(decoderTrainPaddedSequence[:20],0),tf.float32)\n",
    "# maskDecoder=maskDecoder[:,tf.newaxis,tf.newaxis,:]\n",
    "# lookAheadMask=tf.linalg.band_part(tf.ones((maxDecodingLen,maxDecodingLen)),-1,0)\n",
    "# maskDecoder=tf.minimum(maskDecoder,lookAheadMask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionVocabSize=len(questionToken.word_index)+1\n",
    "replyVocabSize=len(replyToken.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200, 17245, 14464)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxEncodingLen,maxDecodingLen,questionVocabSize,replyVocabSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIMENSIONS=100\n",
    "NUM_OF_HEADS=2\n",
    "HIDDEN_DIMENSIONS=300\n",
    "NUM_OF_BLOCKS_ENCODER=2\n",
    "NUM_OF_BLOCKS_DECODER=6\n",
    "DROPOUT=0.2\n",
    "\n",
    "BATCH_SIZE=8\n",
    "EPOCHS=5\n",
    "\n",
    "INITIAL_LEARNING_RATE=0.003\n",
    "DECAY_STEPS=1000\n",
    "DECAY_RATE=0.6\n",
    "STAIRCASE=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaledDotProdect(query,key,value,mask=None):\n",
    "    keyDim=tf.cast(tf.shape(key)[-1],tf.float32)\n",
    "    scores=tf.matmul(query,key,transpose_b=True)/tf.sqrt(keyDim)\n",
    "    if mask is not None:\n",
    "        scores += (mask * -1e9)\n",
    "    softmax=Softmax()\n",
    "    weights=softmax(scores)\n",
    "    \n",
    "    return tf.matmul(weights,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedForward(EMBEDDING_DIMENSIONS,HIDDEN_DIMENSIONS):\n",
    "    return Sequential([\n",
    "        Dense(HIDDEN_DIMENSIONS,activation='selu'),\n",
    "        Dense(EMBEDDING_DIMENSIONS)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadSelfAttention(Layer):\n",
    "    def __init__(self,EMBEDDING_DIMENSIONS,NUM_OF_HEADS):\n",
    "        super (MultiheadSelfAttention,self).__init__()\n",
    "        self.embeddingDimension=EMBEDDING_DIMENSIONS\n",
    "        self.numOfHeads=NUM_OF_HEADS\n",
    "        self.singleHeadDimensions=EMBEDDING_DIMENSIONS//NUM_OF_HEADS\n",
    "        self.weightQuery=Dense(EMBEDDING_DIMENSIONS,activation='linear',kernel_initializer='orthogonal')\n",
    "        self.weightKey=Dense(EMBEDDING_DIMENSIONS,activation='linear',kernel_initializer='orthogonal')\n",
    "        self.weightValue=Dense(EMBEDDING_DIMENSIONS,activation='linear',kernel_initializer='orthogonal')\n",
    "    \n",
    "    def splitHeads(self,input):\n",
    "        batchSize = tf.shape(input)[0] \n",
    "        splitInputs=tf.reshape(input,(batchSize,-1,self.numOfHeads,self.singleHeadDimensions))\n",
    "        return tf.transpose(splitInputs,perm=[0,2,1,3])\n",
    "    \n",
    "    def mergeHeads(self,input):\n",
    "        batchSize = tf.shape(input)[0] \n",
    "        mergedInputs=tf.transpose(input,perm=[0,2,1,3])\n",
    "        return tf.reshape(mergedInputs,(batchSize,-1,self.embeddingDimension))\n",
    "    \n",
    "    def call(self, q,k,v,mask=None):\n",
    "\n",
    "        query=self.weightQuery(q)\n",
    "        key=self.weightKey(k)\n",
    "        value=self.weightValue(v)\n",
    "        query=self.splitHeads(query)\n",
    "        key=self.splitHeads(key)\n",
    "        value=self.splitHeads(value)\n",
    "        output=scaledDotProdect(query,key,value,mask)\n",
    "        output=self.mergeHeads(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(Layer):\n",
    "    def __init__(self,EMBEDDING_DIMENSIONS,NUM_OF_HEADS,HIDDEN_DIMENSIONS,DROPOUT=0.1):\n",
    "        super(EncoderBlock,self).__init__()\n",
    "\n",
    "        self.mhsa=MultiheadSelfAttention(EMBEDDING_DIMENSIONS,NUM_OF_HEADS)\n",
    "        self.feedForwardLayer=feedForward(EMBEDDING_DIMENSIONS,HIDDEN_DIMENSIONS)\n",
    "        self.dropoutLayer1=Dropout(DROPOUT)\n",
    "        self.dropoutLayer2=Dropout(DROPOUT)\n",
    "        self.layerNorm1=LayerNormalization()\n",
    "        self.layerNorm2=LayerNormalization()\n",
    "    \n",
    "    def call(self,input,training,mask=None):\n",
    "        \n",
    "        mhsaOut=self.mhsa(input,input,input,mask)\n",
    "        mhsaOut=self.dropoutLayer1(mhsaOut,training=training)\n",
    "        mhsaOut=self.layerNorm1(input+mhsaOut)\n",
    "\n",
    "        feedOut=self.feedForwardLayer(mhsaOut)\n",
    "        feedOut=self.dropoutLayer2(feedOut, training=training)\n",
    "        feedOut=self.layerNorm2(input+feedOut)\n",
    "\n",
    "        return feedOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Layer):\n",
    "    def __init__(self,questionVocabSize,EMBEDDING_DIMENSIONS,maxEncodingLen,NUM_OF_HEADS,HIDDEN_DIMENSIONS,NUM_OF_BLOCKS_ENCODER,DROPOUT=0.1):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.maxEncodingLen=maxEncodingLen\n",
    "        self.embeddingDimensions=EMBEDDING_DIMENSIONS\n",
    "        self.tokenEncodingLayer=Embedding(questionVocabSize,self.embeddingDimensions)\n",
    "        self.positionEncodingLayer=Embedding(self.maxEncodingLen,self.embeddingDimensions)\n",
    "        \n",
    "        self.dropoutLayer=Dropout(DROPOUT)\n",
    "        self.blocks=[EncoderBlock(self.embeddingDimensions,NUM_OF_HEADS,HIDDEN_DIMENSIONS) for _ in range(NUM_OF_BLOCKS_ENCODER)]\n",
    "\n",
    "    def call(self, input, training, mask=None):\n",
    "        tokenEncodings=self.tokenEncodingLayer(input)\n",
    "        batch_size = tf.shape(input)[0] \n",
    "        # numPos=batch_size*self.maxEncodingLen\n",
    "        posIndex = tf.range(self.maxEncodingLen)\n",
    "        posIndex = tf.broadcast_to(posIndex, [batch_size, self.maxEncodingLen])\n",
    "        posEncodings=self.positionEncodingLayer(posIndex)\n",
    "        input=self.dropoutLayer(posEncodings+tokenEncodings)\n",
    "        for block in self.blocks:\n",
    "            input=block(input,training,mask)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(Layer):\n",
    "    def __init__(self,EMBEDDING_DIMENSIONS,NUM_OF_HEADS,HIDDEN_DIMENSIONS,DROPOUT=0.1):\n",
    "        super(DecoderBlock,self).__init__()\n",
    "        self.mhsa1=MultiheadSelfAttention(EMBEDDING_DIMENSIONS,NUM_OF_HEADS)\n",
    "        self.mhsa2=MultiheadSelfAttention(EMBEDDING_DIMENSIONS,NUM_OF_HEADS)\n",
    "        self.feedForwardLayer=feedForward(EMBEDDING_DIMENSIONS,HIDDEN_DIMENSIONS)\n",
    "        self.dropoutLayer1=Dropout(DROPOUT)\n",
    "        self.dropoutLayer2=Dropout(DROPOUT)\n",
    "        self.dropoutLayer3=Dropout(DROPOUT)\n",
    "        self.layerNorm1=LayerNormalization()\n",
    "        self.layerNorm2=LayerNormalization()\n",
    "        self.layerNorm3=LayerNormalization()\n",
    "\n",
    "    def call(self, encoderOut,target, training, decoderMask,memoryMask):\n",
    "        mhsaOut1=self.mhsa1(target,target,target,decoderMask)\n",
    "        mhsaOut1=self.dropoutLayer1(mhsaOut1,training=training)\n",
    "        mhsaOut1=self.layerNorm1(mhsaOut1+target)\n",
    "\n",
    "        mhsaOut2=self.mhsa2(mhsaOut1,encoderOut,encoderOut,memoryMask)\n",
    "        mhsaOut2=self.dropoutLayer2(mhsaOut2,training=training)\n",
    "        mhsaOut2=self.layerNorm2(mhsaOut1+mhsaOut2)\n",
    "\n",
    "        feedOut=self.feedForwardLayer(mhsaOut2)\n",
    "        feedOut=self.dropoutLayer3(feedOut,training=training)\n",
    "        output=self.layerNorm3(feedOut+mhsaOut2)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Layer):\n",
    "    def __init__(self,replyVocabSize,EMBEDDING_DIMENSIONS,NUM_OF_HEADS,HIDDEN_DIMENSIONS,NUM_OF_BLOCKS_DECODER,maxDecodingLen,DROPOUT=0.1):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.maxDecodingLen=maxDecodingLen\n",
    "        self.embeddingDimensions=EMBEDDING_DIMENSIONS\n",
    "        self.tokenEncodingLayer=Embedding(replyVocabSize,self.embeddingDimensions)\n",
    "        self.positionEncodingLayer=Embedding(self.maxDecodingLen,self.embeddingDimensions)\n",
    "        self.dropout=Dropout(DROPOUT)\n",
    "        self.blocks=[DecoderBlock(self.embeddingDimensions,NUM_OF_HEADS,HIDDEN_DIMENSIONS,DROPOUT) for _ in range(NUM_OF_BLOCKS_DECODER)]\n",
    "\n",
    "    def call(self, encoderOut,target,training,decoderMask,memoryMask):\n",
    "        tokenEmbeddings=self.tokenEncodingLayer(target)\n",
    "        batch_size = tf.shape(target)[0]\n",
    "        posIndex = tf.range(target.shape[1]) \n",
    "        posIndex = tf.broadcast_to(posIndex, [batch_size, target.shape[1]])\n",
    "        posEncodings=self.positionEncodingLayer(posIndex)\n",
    "        target=self.dropout(posEncodings+tokenEmbeddings,training=training)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            target=block(encoderOut,target,training,decoderMask,memoryMask)\n",
    "        return target\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(Model):\n",
    "    def __init__(self, questionVocabSize,replyVocabSize,EMBEDDING_DIMENSIONS,maxEncodingLen,maxDecodingLen,NUM_OF_HEADS,HIDDEN_DIMENSIONS,NUM_OF_BLOCKS_ENCODER,NUM_OF_BLOCKS_DECODER,dropout=0.1):\n",
    "        super(Transformer,self).__init__()\n",
    "        self.encoder=Encoder(questionVocabSize,EMBEDDING_DIMENSIONS,maxEncodingLen,NUM_OF_HEADS,HIDDEN_DIMENSIONS,NUM_OF_BLOCKS_ENCODER,dropout)\n",
    "        self.decoder=Decoder(replyVocabSize,EMBEDDING_DIMENSIONS,NUM_OF_HEADS,HIDDEN_DIMENSIONS,NUM_OF_BLOCKS_DECODER,maxDecodingLen,dropout)\n",
    "\n",
    "        self.finalLayer=Dense(replyVocabSize)\n",
    "    \n",
    "    def call(self,input,target,training=False):\n",
    "        encoderMask=tf.cast(tf.math.not_equal(input,0),tf.float32)\n",
    "        encoderMask=encoderMask[:,tf.newaxis,tf.newaxis,:]\n",
    "        \n",
    "        decoderMask=tf.cast(tf.math.not_equal(target,0),tf.float32)\n",
    "        decoderMask=decoderMask[:,tf.newaxis,tf.newaxis,:]\n",
    "        lookAheadMask=tf.linalg.band_part(tf.ones((target.shape[1],target.shape[1])),-1,0)\n",
    "        decoderMask=tf.minimum(decoderMask,lookAheadMask)\n",
    "\n",
    "        encoderOut=self.encoder(input,mask=encoderMask,training=training)\n",
    "        decoderOut=self.decoder(encoderOut,target,training=training,decoderMask=decoderMask,memoryMask=encoderMask)\n",
    "\n",
    "        output=self.finalLayer(decoderOut)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customLoss(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss= SparseCategoricalCrossentropy(from_logits=True, reduction='none')(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask \n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(Model):\n",
    "    def __init__(self,transformer,optimizer,customLossFunction):\n",
    "        super(Trainer,self).__init__()\n",
    "        self.transformer=transformer\n",
    "        self.optimizer=optimizer\n",
    "        self.customLossFunction=customLossFunction\n",
    "        self.lossAtEachStep=[]\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        input, target=inputs\n",
    "        targetInput = target[:, :-1]\n",
    "        targetOutput = target[:, 1:]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.transformer(input, targetInput, training=True)\n",
    "            loss = self.customLossFunction(targetOutput, predictions)\n",
    "            self.lossAtEachStep.append(loss)\n",
    "        gradients = tape.gradient(loss, self.transformer.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.transformer.trainable_variables))\n",
    "\n",
    "        return {'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel():\n",
    "    return Transformer(questionVocabSize,\n",
    "                        replyVocabSize,\n",
    "                        EMBEDDING_DIMENSIONS,\n",
    "                        maxEncodingLen,\n",
    "                        maxDecodingLen,\n",
    "                        NUM_OF_HEADS,\n",
    "                        HIDDEN_DIMENSIONS,\n",
    "                        NUM_OF_BLOCKS_ENCODER,\n",
    "                        NUM_OF_BLOCKS_DECODER,\n",
    "                        DROPOUT)\n",
    "\n",
    "transformer=createModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=keras.optimizers.Adam()\n",
    "schedule=ExponentialDecay(initial_learning_rate=INITIAL_LEARNING_RATE,decay_steps= DECAY_STEPS,decay_rate= DECAY_RATE,staircase=STAIRCASE)\n",
    "trainer=Trainer(transformer,optimizer,customLoss)\n",
    "trainer.compile(optimizer=optimizer,loss=customLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_1 (Encoder)         multiple                  1926700   \n",
      "                                                                 \n",
      " decoder_1 (Decoder)         multiple                  2196000   \n",
      "                                                                 \n",
      " dense_117 (Dense)           multiple                  1460864   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,583,564\n",
      "Trainable params: 5,583,564\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer(encoderTrainPaddedSequence[:1],decoderTrainPaddedSequence[:1])\n",
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1250/1250 [==============================] - 453s 352ms/step - loss: 5.8707 - lr: 0.0030\n",
      "Epoch 2/5\n",
      "1250/1250 [==============================] - 448s 359ms/step - loss: 5.8088 - lr: 0.0030\n",
      "Epoch 3/5\n",
      "1250/1250 [==============================] - 451s 361ms/step - loss: 5.7997 - lr: 0.0030\n",
      "Epoch 4/5\n",
      "1250/1250 [==============================] - 450s 360ms/step - loss: 5.7952 - lr: 0.0030\n",
      "Epoch 5/5\n",
      "1250/1250 [==============================] - 448s 359ms/step - loss: 5.7919 - lr: 0.0030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24a803a9850>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetTrain=tf.data.Dataset.from_tensor_slices((encoderTrainPaddedSequence[:10000],decoderTrainPaddedSequence[:10000])).batch(BATCH_SIZE)\n",
    "trainer.fit(datasetTrain,epochs=EPOCHS,callbacks=LearningRateScheduler(schedule))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del transformer\n",
    "# del trainer\n",
    "# keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i ' ve been feeling so sad and overwhelmed lately work has become such a massive source of stress for me <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questionToken.sequences_to_texts(encoderTrainPaddedSequence[:1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you you you you you you you you you you you you you you you you you you you you you you "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9100\\1249531830.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mresult_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresult_sentence\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult_sentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mquestionToken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequences_to_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoderTrainPaddedSequence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0moutput_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestionToken\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreplyToken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxDecodingLen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmaxEncodingLen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Output:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9100\\1249531830.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(input_sentence, transformerModel, qtokenizer, rtokenizer, maxDecodingLen, maxEncodingLen)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mresult_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mrtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'<start>'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0msoftmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxDecodingLen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mtarget_sequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mresult_sentence\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformerModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mpossibilities\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mpredicted_token\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibilities\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    554\u001b[0m                 \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mcopied_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcopied_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m             \u001b[0mlayout_map_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_subclass_model_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layout_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m                 with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1143\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1144\u001b[0m                 ):\n\u001b[1;32m-> 1145\u001b[1;33m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1148\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9100\\1970228160.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, input, target, training)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mlookAheadMask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mband_part\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mdecoderMask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoderMask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlookAheadMask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mencoderOut\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoderMask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mdecoderOut\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoderOut\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoderMask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoderMask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmemoryMask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoderMask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinalLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoderOut\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m                 with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1143\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1144\u001b[0m                 ):\n\u001b[1;32m-> 1145\u001b[1;33m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1148\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9100\\1500760736.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, encoderOut, target, training, decoderMask, memoryMask)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mposEncodings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpositionEncodingLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposEncodings\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtokenEmbeddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoderOut\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoderMask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmemoryMask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m                 with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1143\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1144\u001b[0m                 ):\n\u001b[1;32m-> 1145\u001b[1;33m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1148\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9100\\3966601666.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, encoderOut, target, training, decoderMask, memoryMask)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoderOut\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoderMask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmemoryMask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mmhsaOut1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmhsa1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoderMask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mmhsaOut1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropoutLayer1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmhsaOut1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mmhsaOut1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayerNorm1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmhsaOut1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m                 with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1143\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1144\u001b[0m                 ):\n\u001b[1;32m-> 1145\u001b[1;33m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1148\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9100\\420601722.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, q, k, v, mask)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mquery\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitHeads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitHeads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitHeads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaledDotProdect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmergeHeads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9100\\420601722.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmergeHeads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mbatchSize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mmergedInputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mperm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmergedInputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddingDimension\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1174\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[1;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[0;32m   1110\u001b[0m           \u001b[0mpacked_strides\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpacked_strides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m       \u001b[0mvar_empty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m       \u001b[0mpacked_begin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpacked_end\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpacked_strides\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvar_empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m     return strided_slice(\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m         \u001b[0mpacked_begin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1117\u001b[0m         \u001b[0mpacked_end\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1174\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[1;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mstrides\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m     \u001b[0mstrides\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1287\u001b[1;33m   op = gen_array_ops.strided_slice(\n\u001b[0m\u001b[0;32m   1288\u001b[0m       \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1289\u001b[0m       \u001b[0mbegin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1290\u001b[0m       \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[0;32m  13530\u001b[0m         \"new_axis_mask\", new_axis_mask, \"shrink_axis_mask\", shrink_axis_mask)\n\u001b[0;32m  13531\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  13532\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  13533\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 13534\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  13535\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  13536\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  13537\u001b[0m       return strided_slice_eager_fallback(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def inference(input_sentence, transformerModel, qtokenizer,rtokenizer, maxDecodingLen,maxEncodingLen):\n",
    "    input_sequence = qtokenizer.texts_to_sequences([input_sentence])\n",
    "    input_sequence = pad_sequences(input_sequence, maxlen=maxEncodingLen, padding='post')\n",
    "    input_tensor = tf.convert_to_tensor(input_sequence)\n",
    "    result_sentence = [rtokenizer.word_index['<start>']]\n",
    "    softmax=Softmax()\n",
    "    for i in range(maxDecodingLen):\n",
    "        target_sequence = tf.convert_to_tensor([result_sentence])\n",
    "        output = transformerModel(input_tensor, target_sequence, training=False)\n",
    "        logits=output[:, -1, :]\n",
    "        possibilities=softmax(logits)\n",
    "        predicted_token =np.argmax(possibilities,axis=-1)[0]\n",
    "        result_sentence.append(predicted_token)\n",
    "        if predicted_token == rtokenizer.word_index['<end>']:\n",
    "            break\n",
    "        print(rtokenizer.index_word[predicted_token],end=\" \")\n",
    "    result_sentence = ' '.join([rtokenizer.index_word[token] for token in result_sentence if token not in [0]])\n",
    "    return result_sentence\n",
    "\n",
    "input_sentence =questionToken.sequences_to_texts(encoderTrainPaddedSequence[:1])[0]\n",
    "output_sentence = inference(input_sentence, transformer, questionToken,replyToken, maxDecodingLen,maxEncodingLen)\n",
    "print(\"Input:\", input_sentence)\n",
    "print(\"Output:\", output_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRoot='Models/TransformerMark-'\n",
    "modelIndex=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(f'{modelRoot}{modelIndex}')\n",
    "except:\n",
    "    pass\n",
    "transformer.save_weights(f'{modelRoot}{modelIndex}/Transformer.h5')\n",
    "with open(f'{modelRoot}{modelIndex}/Model-Architecture.json', 'w') as f:\n",
    "    f.write(transformer.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionDict = {\n",
    "    'class_name': 'QuestionTokenizer',\n",
    "    'num_words' :questionToken.num_words,\n",
    "    'filters'   :questionToken.filters,\n",
    "    'lower'     :questionToken.lower,\n",
    "    'split'     :questionToken.split,\n",
    "    'char_level':questionToken.char_level,\n",
    "    'oov_token' :questionToken.oov_token,\n",
    "    'document_count':questionToken.document_count,\n",
    "    'word_counts':questionToken.word_counts,\n",
    "    'word_docs':questionToken.word_docs,\n",
    "    'index_docs':questionToken.index_docs,\n",
    "    'index_word':questionToken.index_word,\n",
    "    'word_index':questionToken.word_index\n",
    "}\n",
    "replyDict = {\n",
    "    'class_name': 'ReplyTokenizer',\n",
    "    'num_words' :replyToken.num_words,\n",
    "    'filters'   :replyToken.filters,\n",
    "    'lower'     :replyToken.lower,\n",
    "    'split'     :replyToken.split,\n",
    "    'char_level':replyToken.char_level,\n",
    "    'oov_token' :replyToken.oov_token,\n",
    "    'document_count':replyToken.document_count,\n",
    "    'word_counts':replyToken.word_counts,\n",
    "    'word_docs':replyToken.word_docs,\n",
    "    'index_docs':replyToken.index_docs,\n",
    "    'index_word':replyToken.index_word,\n",
    "    'word_index':replyToken.word_index\n",
    "}\n",
    "\n",
    "qJ=json.dumps(questionDict)\n",
    "rJ=json.dumps(replyDict)\n",
    "with open(\"Dictionary/QuestionDictionary.json\", \"w\") as outfile:\n",
    "    outfile.write(qJ)\n",
    "\n",
    "with open(\"Dictionary/ReplyDictionary.json\", \"w\") as outfile:\n",
    "    outfile.write(rJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    \"EMBEDDING_DIMENSIONS\": EMBEDDING_DIMENSIONS,\n",
    "    \"NUM_OF_HEADS\":NUM_OF_HEADS,\n",
    "    \"HIDDEN_DIMENSIONS\":HIDDEN_DIMENSIONS,\n",
    "    \"NUM_OF_BLOCKS_ENCODER\":NUM_OF_BLOCKS_ENCODER,\n",
    "    \"NUM_OF_BLOCKS_DECODER\":NUM_OF_BLOCKS_DECODER,\n",
    "    \"DROPOUT\":DROPOUT,\n",
    "    \"BATCH_SIZE\":BATCH_SIZE,\n",
    "    \"EPOCHS\":EPOCHS,\n",
    "    \"INITIAL_LEARNING_RATE\":INITIAL_LEARNING_RATE,\n",
    "    \"DECAY_STEPS\":DECAY_STEPS,\n",
    "    \"DECAY_RATE\":DECAY_RATE,\n",
    "    'questionVocabSize':questionVocabSize,\n",
    "    'replyVocabSize':replyVocabSize,\n",
    "    'maxEncodingLen':maxEncodingLen,\n",
    "    'maxDecodingLen':maxDecodingLen,\n",
    "    'encoderSequenceShape':encoderTrainPaddedSequence[:1].shape,\n",
    "    'decoderSequenceShape':decoderTrainPaddedSequence[:1].shape,\n",
    "    'modelRoot':'Models/TransformerMark-',\n",
    "    'modelIndex':modelIndex\n",
    "}\n",
    "tokenizer={\n",
    "        'questionTokenizor':questionToken.to_json(),\n",
    "        'replyTokenizor':replyToken.to_json()\n",
    "}\n",
    "paramsJson=json.dumps(params)\n",
    "with open('Application/Parameters/HyperParameters.json','w') as file:\n",
    "    file.write(paramsJson)\n",
    "tokenizerJson=json.dumps(tokenizer)\n",
    "with open('Application/Parameters/Tokenizor.json','w') as file:\n",
    "    file.write(tokenizerJson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading The Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Models/TransformerMark-2/Parameters/HyperParameters.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[133], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodelRoot\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodelIndex\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/Parameters/HyperParameters.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      2\u001b[0m     hParams\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodelRoot\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmodelIndex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/Parameters/Tokenizor.json\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Models/TransformerMark-2/Parameters/HyperParameters.json'"
     ]
    }
   ],
   "source": [
    "with open(f'{modelRoot}{modelIndex}/Parameters/HyperParameters.json','r') as file:\n",
    "    hParams=json.load(file)\n",
    "with open(f'{modelRoot}{modelIndex}/Parameters/Tokenizor.json','r') as file:\n",
    "    tokenizor=json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionTokenizor=tokenizer_from_json(tokenizer['questionTokenizor'])\n",
    "replyTokenizor=tokenizer_from_json(tokenizer['replyTokenizor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadedTransformer=createModel()\n",
    "customObject={\"Transformer\":loadedTransformer}\n",
    "with open(f\"{hParams['modelRoot']}{hParams['modelIndex']}/Model-Architecture.json\", 'r') as f:\n",
    "    modelArchitectureJson = f.read()\n",
    "loadedTransformer= model_from_json(modelArchitectureJson,custom_objects=customObject)\n",
    "inp=np.random.randint(1,size=hParams['encoderSequenceShape'])\n",
    "out=np.random.randint(1,size=hParams['decoderSequenceShape'])\n",
    "loadedTransformer(inp,out)\n",
    "loadedTransformer.load_weights(f\"{hParams['modelRoot']}{hParams['modelIndex']}/Transformer.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadedTransformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=keras.optimizers.Adam()\n",
    "schedule=ExponentialDecay(initial_learning_rate=INITIAL_LEARNING_RATE,decay_steps= DECAY_STEPS,decay_rate= DECAY_RATE,staircase=STAIRCASE)\n",
    "trainer=Trainer(loadedTransformer,optimizer,customLoss)\n",
    "trainer.compile(optimizer=optimizer,loss=customLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet2=pd.read_csv(dataSetRoot+fileName2)\n",
    "# dataSet=dataSet2\n",
    "dataSet3=pd.read_csv(dataSetRoot+fileName3)\n",
    "# dataSet=dataSet3\n",
    "dataSet4=pd.read_csv(dataSetRoot+'DataSet4')\n",
    "dataSet=dataSet4\n",
    "dataSet=pd.concat([dataSet2,dataSet3,dataSet4],ignore_index=True)\n",
    "question=dataSet['question'].tolist()\n",
    "reply=dataSet['reply'].tolist()\n",
    "questionTrain,questionTest,replyTrain,replyTest=train_test_split(question,reply,test_size=0.0001,shuffle=True,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionTrainSequence=questionToken.texts_to_sequences(questionTrain)\n",
    "replyTrainSequence=replyToken.texts_to_sequences(tagging(replyTrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoderTrainPaddedSequence=pad_sequences(sequences=questionTrainSequence,maxlen=maxEncodingLen,padding='post',truncating='post')\n",
    "decoderTrainPaddedSequence=pad_sequences(sequences=replyTrainSequence,maxlen=maxDecodingLen,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462/462 [==============================] - 480s 1s/step - loss: 5.4125 - lr: 0.0050\n",
      "Epoch 2/3\n",
      "462/462 [==============================] - 487s 1s/step - loss: 5.1445 - lr: 0.0050\n",
      "Epoch 3/3\n",
      "462/462 [==============================] - 486s 1s/step - loss: 4.9869 - lr: 0.0050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b98357efd0>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetTrain=tf.data.Dataset.from_tensor_slices((encoderTrainPaddedSequence,decoderTrainPaddedSequence)).batch(BATCH_SIZE)\n",
    "trainer.fit(datasetTrain,epochs=EPOCHS,callbacks=LearningRateScheduler(schedule))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"i ' m having relationship problems and i want to fix them to make things right before i lose her how can i learn how to listen and get myself together ? <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\"]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questionToken.sequences_to_texts(encoderTrainPaddedSequence[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replyTokenizor.sequences_to_texts(decoderTrainPaddedSequence[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i ' t have a lot of your question is a lot of your question is a lot of your husband is a lot of your husband is a lot of your husband is a lot of your husband is a lot of your husband is a lot of your husband ' t have a lot of your husband is a lot of your husband is a lot of your husband is a lot of your husband is a lot of your husband is a lot of your husband is a lot of your husband is a lot of your husband ' t have a lot of your husband is a lot of your husband is a lot of the client can be helpful to be helpful to be helpful to be helpful to be helpful to be helpful to be helpful to be helpful to be helpful to be helpful to be helpful to be helpful to be helpful to be helpful to be helpful to be helpful to be helpful to be helpful to be a lot of the client is a lot of the client is a lot of the client is a lot of the client Input: [\"i ' m having relationship problems and i want to fix them to make things right before i lose her how can i learn how to listen and get myself together ? <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\"]\n",
      "Output: <start> i ' t have a lot of your question is a lot of your question is a lot of your husband is a lot of your husband is a lot of your husband is a lot of your husband is a lot of your husband is a lot of your husband ' t have a lot of your husband is a lot of your husband is a lot of your husband is a lot of your husband is a lot of your husband is a lot of your husband is a lot of your husband is a lot of your husband ' t have a lot of your husband is a lot of your husband is a lot of the client can be helpful to be helpful to be helpful to be helpful to be helpful to be helpful to be helpful to be helpful to be helpful to be helpful to be helpful to be helpful to be helpful to be helpful to be helpful to be helpful to be helpful to be helpful to be a lot of the client is a lot of the client is a lot of the client is a lot of the client\n"
     ]
    }
   ],
   "source": [
    "input_sentence = questionToken.sequences_to_texts(encoderTrainPaddedSequence[1:2])\n",
    "output_sentence = inference(input_sentence, transformer, questionToken,replyToken, maxDecodingLen,maxEncodingLen)\n",
    "print(\"Input:\", input_sentence)\n",
    "print(\"Output:\", output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
